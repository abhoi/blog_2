---
layout: post
title: Introduction to Conditional Random Fields
---

<p align="justify">Conditional Random Fields (CRFs) are a class of statistical modeling method often to used to model structured data. CRFs are widely used in machine learning problems. Some specific applications include:</p>

- **Computer Vision**: Object recognition, image segmentation
- **Natural Language Processing**: POS tagging, Shallow parsing, Named entity recognition
- **Biological Sequences**: Gene finding, Peptide critical functional region finding

CRFs are used as an alternative to hidden Markov models (HMMs) as well. Conditional Random Fields were introduced in 2001 by Lafferty, John, Andrew McCallum, and Fernando CN Pereira [2].

A **conditional random field** or **CRF** is a **Markov Random Field** where all the clique potentials are conditioned on input features:

$$p(\textbf{y}|\textbf{x, w}) = \frac{1}{Z(\textbf{x, w})}\prod_{c} \psi_{c}(\textbf{y}_{c}|\textbf{x, w})$$

A CRF is basically a **structured ouput** extension of **logistic regression**. We can assume a log-linear representation of the potentials:

$$\psi_{c}(\textbf{y}_{c}|\textbf{x, w}) = exp(\textbf{w}_{c}^{T} \phi(\textbf{x, }\textbf{y}_{c}))$$

<p align='justify'>CRFs are more efficient than generative classifiers because they don't waste resources modeling things that they always observe. They only focus on the distribution of labels given data. This leads to the potentials of the model be data-dependent. If two nodes <i>x</i> and <i>y</i> are discontinuous, then we can "turn off" or set the transition potential to zero. [1]</p>

We can also represent CRFs as:

$$p(\textbf{y}|X) = \frac{1}{Z_X}\textrm{exp}\left (\sum_{s=1}^{m} \left \langle \textbf{w}_{y_{s}}, \textbf{x}_{s} \right \rangle + \sum_{s=1}^{m-1}T_{y_{s},y_{s+1}}\right)$$

$$Z_X = \sum_{\mathbf{\hat{y}} \in \mathcal{Y}^{m}}^{ } \textrm{exp}\left (\sum_{s=1}^{m} \left \langle \textbf{w}_{\hat{y}_{s}}, \textbf{x}_{s} \right \rangle + \sum_{s=1}^{m-1}T_{\hat{y}_{s},\hat{y}_{s+1}}\right )$$

<p align='justify'>A key advantange of CRFs over Markov Random Fields (MRF) is that we don't "waste resources" modeling things that we always observe. Another advantage is that the potentials of the model are data-dependent. As an example, if we are classifying images, we can "turn off" label smoothing between two neighboring nodes $s$ and $t$. The only disadvantage of CRF over MRF is that they require labeled training data and are slower to train.</p>

<p align='justify'>The most commonly used CRFs use a chain-structured graph to model correlation amongst neighboring labels. Traditionally, Hidden Markov Models (HMMs) are used for such tasks. These are joint density models of form:</p>

$$p(\mathbf{x}, \mathbf{y}|\mathbf{w}) = \prod_{t=1}^{T}p(y_{t}|y_{t-1}, \mathbf{w})p(\mathbf{x}_{t}|y_{t},\mathbf{w})$$

<p align='justify'>An HMM requires specifying a generative model which is difficult. Furthermore, each $\mathbf{x}_{t}$ is required to be local, since it is hard to define a generative model for the whole stream of observations $\mathbf{x}=\mathbf{x}_{1:T}$. An obvious way to make a descriminative version of an HMM is to "reverse the arrows" from $y_{t}$ to $\mathbf{x}_{t}$ to model a form as:</p>

$$p(\mathbf{x}, \mathbf{y}|\mathbf{w}) = \prod_{t}p(y_{t}|y_{t-1}, \mathbf{x},\mathbf{w})$$

<p align='justify'>This is called a <b>maximum entropy Markov model</b> or <b>MEMM</b>. An MEMM is simply a Markov chain whose state transition probabilities are conditioned on input features. A chain-structured CRF has the form:</p>

$$p(\mathbf{y}|\mathbf{x}, \mathbf{w}) = \frac{1}{Z(\mathbf{x}, \mathbf{w})}\psi (y_t|\mathbf{x}, \mathbf{w})\prod_{t=1}^{T-1}\psi(y_t, y_{t+1}|\mathbf{x}, \mathbf{w})$$

<p align='justify'>Chain-structured CRFs remove the <b>label bias</b> problem. The problem is that local features at time $t$ do not influence states prior to time $t$. In MEMMs, the label bias problem occurs because directed models are <b>locally normalized</b>, meaning each CPD sums to 1. By contrast, MRFs and CRFs are <b>globally normalized</b>, which means that local factors do not need to sum to 1, since partition function $Z$, which sums over all joint configurations, will ensure the model defines a valid distribution. The main disadvantage is that we do not get a valid probability distribution over $\mathbf{y}$ until we have seen the whole sentence. CRFs are also not as useful as DGMs for online or real-time inference. Also, $Z$ depends on all nodes which makes CRFs much slower to train.</p>

**References**:

1. Murphy, Kevin P. "Machine Learning: A Probabilistic Perspective. Adaptive Computation and Machine Learning." (2012).
2. Lafferty, John, Andrew McCallum, and Fernando CN Pereira. "Conditional random fields: Probabilistic models for segmenting and labeling sequence data." (2001).